import os
import random
import shutil

import matplotlib.pyplot as plt
import numpy as np

from Config import Config
from D3QN import DuelingDoubleDeepQNetwork
from MEC_Env import MEC

"""
if not os.path.exists("models"):
    os.mkdir("models")
else:
    shutil.rmtree("models")
    os.mkdir("models")
"""


def normalize(parameter, minimum, maximum):
    normalized_parameter = (parameter - minimum) / (maximum - minimum)
    return normalized_parameter


def QoE_Function(
    delay,
    max_delay,
    unfinish_task,
    ue_energy_state,
    ue_comp_energy,
    ue_trans_energy,
    edge_comp_energy,
    ue_idle_energy,
):

    edge_energy = next((e for e in edge_comp_energy if e != 0), 0)
    idle_energy = next((e for e in ue_idle_energy if e != 0), 0)

    energy_cons = ue_comp_energy + ue_trans_energy  # + edge_energy + idle_energy
    # print(ue_comp_energy , ue_trans_energy , edge_energy , idle_energy)
    # print(ue_energy_state, delay, energy_cons)

    scaled_energy = normalize(energy_cons, 0, 20) * 10
    cost = 2 * ((ue_energy_state * delay) + ((1 - ue_energy_state) * scaled_energy))

    Reward = max_delay * 4

    if unfinish_task:
        QoE = -cost
    else:
        QoE = Reward - cost

    return QoE


def Drop_Count(ue_RL_list, episode):
    # print(env.unfinish_task.shape)
    drrop_delay10 = 0
    drrop = 0
    for time_index in range(100):
        drrop = drrop + sum(env.unfinish_task[time_index])

    for i in range(len(ue_RL_list)):
        for j in range(len(ue_RL_list[i].delay_store[episode])):
            if ue_RL_list[i].delay_store[episode][j] == 10:
                drrop_delay10 = drrop_delay10 + 1

    # print("-----------", drrop_delay10, drrop)
    return drrop


def Cal_QoE(ue_RL_list, episode):
    episode_sum_reward = sum(sum(ue_RL.reward_store[episode]) for ue_RL in ue_RL_list)
    avg_episode_sum_reward = episode_sum_reward / len(ue_RL_list)
    # print(f"reward: {avg_episode_sum_reward}")
    return avg_episode_sum_reward


def Cal_Delay(ue_RL_list, episode):

    avg_delay_in_episode = []
    for i in range(len(ue_RL_list)):
        for j in range(len(ue_RL_list[i].delay_store[episode])):
            if ue_RL_list[i].delay_store[episode][j] != 0:
                avg_delay_in_episode.append(ue_RL_list[i].delay_store[episode][j])
    avg_delay_in_episode = sum(avg_delay_in_episode) / len(avg_delay_in_episode)
    return avg_delay_in_episode


def Cal_Energy(ue_RL_list, episode):
    energy_ue_list = [sum(ue_RL.energy_store[episode]) for ue_RL in ue_RL_list]
    avg_energy_in_episode = sum(energy_ue_list) / len(energy_ue_list)
    # print(f"energy: {avg_energy_in_episode}")
    return avg_energy_in_episode


"""
def cal_reward(ue_RL_list):
    total_sum_reward = 0
    num_episodes = 0
    for ue_num, ue_RL in enumerate(ue_RL_list):
        print("________________________")
        print("ue_num:", ue_num)
        print("________________________")
        for episode, reward in enumerate(ue_RL.reward_store):
            print("episode:", episode)
            reward_sum = sum(reward)
            print(reward_sum)
            total_sum_reward += reward_sum
            num_episodes += 1
    avg_reward = total_sum_reward / num_episodes
    print(total_sum_reward, avg_reward)

"""


def train(ue_RL_list, NUM_EPISODE):
    avg_QoE_list = []
    avg_delay_list = []
    energy_cons_list = []
    num_drop_list = []
    avg_reward_list = []
    avg_reward_list_2 = []
    avg_delay_list_in_episode = []
    avg_energy_list_in_episode = []
    num_task_drop_list_in_episode = []
    RL_step = 0
    a = 1

    for episode in range(NUM_EPISODE):

        print("\n-*-**-***-*****-********-*************-********-*****-***-**-*-")
        print(
            "Episode  :",
            episode,
        )
        print("Epsilon  :", ue_RL_list[0].epsilon)

        # BITRATE ARRIVAL
        bitarrive_size = np.random.uniform(
            env.min_arrive_size, env.max_arrive_size, size=[env.n_time, env.n_ue]
        )
        task_prob = env.task_arrive_prob
        bitarrive_size = bitarrive_size * (
            np.random.uniform(0, 1, size=[env.n_time, env.n_ue]) < task_prob
        )
        bitarrive_size[-env.max_delay :, :] = np.zeros([env.max_delay, env.n_ue])

        bitarrive_dens = np.zeros([env.n_time, env.n_ue])
        for i in range(len(bitarrive_size)):
            for j in range(len(bitarrive_size[i])):
                if bitarrive_size[i][j] != 0:
                    bitarrive_dens[i][j] = Config.TASK_COMP_DENS[
                        np.random.randint(0, len(Config.TASK_COMP_DENS))
                    ]

        test = 0
        for i in range(len(bitarrive_size)):
            for j in range(len(bitarrive_size[i])):
                if bitarrive_size[i][j] != 0:
                    test = test + 1

        print("Num_Task_Arrive: ", test)

        Check = []
        for i in range(len(bitarrive_size)):
            Check.append(sum(bitarrive_size[i]))

        # print("Sum_Arrived_Task_Size:", int(sum(Check)))

        # print(bitarrive_dens) = [Config.TASK_COMP_DENS[np.random.randint(0, len(Config.TASK_COMP_DENS))] ]

        # OBSERVATION MATRIX SETTING
        history = list()
        for time_index in range(env.n_time):
            history.append(list())
            for ue_index in range(env.n_ue):
                tmp_dict = {
                    "observation": np.zeros(env.n_features),
                    "lstm": np.zeros(env.n_lstm_state),
                    "action": np.nan,
                    "observation_": np.zeros(env.n_features),
                    "lstm_": np.zeros(env.n_lstm_state),
                }
                history[time_index].append(tmp_dict)
        reward_indicator = np.zeros([env.n_time, env.n_ue])

        # INITIALIZE OBSERVATION
        observation_all, lstm_state_all = env.reset(bitarrive_size, bitarrive_dens)
        # print(observation_all)
        # print(lstm_state_all)

        # TRAIN DRL
        while True:

            # PERFORM ACTION
            action_all = np.zeros([env.n_ue])
            for ue_index in range(env.n_ue):
                observation = np.squeeze(observation_all[ue_index, :])
                if np.sum(observation) == 0:
                    # if there is no task, action = 0 (also need to be stored)
                    action_all[ue_index] = 0
                else:
                    action_all[ue_index] = ue_RL_list[ue_index].choose_action(
                        observation
                    )
                    if observation[0] != 0:
                        ue_RL_list[ue_index].do_store_action(
                            episode, env.time_count, action_all[ue_index]
                        )

            # OBSERVE THE NEXT STATE AND PROCESS DELAY (REWARD)
            observation_all_, lstm_state_all_, done = env.step(action_all)

            # print("+++___+++")
            # print(observation_all_)
            # print(lstm_state_all_)

            # should store this information in EACH time slot
            for ue_index in range(env.n_ue):
                ue_RL_list[ue_index].update_lstm(lstm_state_all_[ue_index, :])

            process_delay = env.process_delay
            unfinish_task = env.unfinish_task

            # STORE MEMORY; STORE TRANSITION IF THE TASK PROCESS DELAY IS JUST UPDATED
            for ue_index in range(env.n_ue):

                history[env.time_count - 1][ue_index]["observation"] = observation_all[
                    ue_index, :
                ]
                history[env.time_count - 1][ue_index]["lstm"] = np.squeeze(
                    lstm_state_all[ue_index, :]
                )
                history[env.time_count - 1][ue_index]["action"] = action_all[ue_index]
                history[env.time_count - 1][ue_index]["observation_"] = (
                    observation_all_[ue_index]
                )
                history[env.time_count - 1][ue_index]["lstm_"] = np.squeeze(
                    lstm_state_all_[ue_index, :]
                )

                update_index = np.where(
                    (1 - reward_indicator[:, ue_index]) * process_delay[:, ue_index] > 0
                )[0]

                if len(update_index) != 0:
                    for update_ii in range(len(update_index)):
                        time_index = update_index[update_ii]
                        ue_RL_list[ue_index].store_transition(
                            history[time_index][ue_index]["observation"],
                            history[time_index][ue_index]["lstm"],
                            history[time_index][ue_index]["action"],
                            QoE_Function(
                                process_delay[time_index, ue_index],
                                env.max_delay,
                                unfinish_task[time_index, ue_index],
                                env.ue_energy_state[ue_index],
                                env.ue_comp_energy[time_index, ue_index],
                                env.ue_tran_energy[time_index, ue_index],
                                env.edge_comp_energy[time_index, ue_index],
                                env.ue_idle_energy[time_index, ue_index],
                            ),
                            history[time_index][ue_index]["observation_"],
                            history[time_index][ue_index]["lstm_"],
                        )
                        ue_RL_list[ue_index].do_store_reward(
                            episode,
                            time_index,
                            QoE_Function(
                                process_delay[time_index, ue_index],
                                env.max_delay,
                                unfinish_task[time_index, ue_index],
                                env.ue_energy_state[ue_index],
                                env.ue_comp_energy[time_index, ue_index],
                                env.ue_tran_energy[time_index, ue_index],
                                env.edge_comp_energy[time_index, ue_index],
                                env.ue_idle_energy[time_index, ue_index],
                            ),
                        )
                        ue_RL_list[ue_index].do_store_delay(
                            episode, time_index, process_delay[time_index, ue_index]
                        )

                        ue_RL_list[ue_index].do_store_energy(
                            episode,
                            time_index,
                            env.ue_comp_energy[time_index, ue_index],
                            env.ue_tran_energy[time_index, ue_index],
                            env.edge_comp_energy[time_index, ue_index],
                            env.ue_idle_energy[time_index, ue_index],
                        )

                        reward_indicator[time_index, ue_index] = 1

            """

            # STORE MEMORY; STORE TRANSITION IF THE TASK PROCESS DELAY IS JUST UPDATED
            for ue_index in range(env.n_ue):
    
                history[env.time_count - 1][ue_index]['observation'] = observation_all[ue_index, :]
                history[env.time_count - 1][ue_index]['lstm'] = np.squeeze(lstm_state_all[ue_index, :])
                history[env.time_count - 1][ue_index]['action'] = action_all[ue_index]
                history[env.time_count - 1][ue_index]['observation_'] = observation_all_[ue_index]
                history[env.time_count - 1][ue_index]['lstm_'] = np.squeeze(lstm_state_all_[ue_index,:])

                update_index = np.where((1 - reward_indicator[:,ue_index]) * env.process_delay[:,ue_index] > 0)[0]


                if len(update_index) != 0:
                    for time_index in range(len(update_index)):
                        reward = QoE_Function(
                            env.ue_comp_energy[time_index, ue_index],
                            env.ue_tran_energy [time_index, ue_index],
                            env.edge_comp_energy[time_index, ue_index],
                            env.ue_idle_energy[time_index, ue_index],
                            env.process_delay[time_index, ue_index],
                            env.max_delay,
                            env.unfinish_task[time_index, ue_index],
                            env.ue_energy_state[ue_index]
                        )
                        ue_RL_list[ue_index].store_transition(
                            history[time_index][ue_index]['observation'],
                            history[time_index][ue_index]['lstm'],
                            history[time_index][ue_index]['action'],
                            reward,
                            history[time_index][ue_index]['observation_'],
                            history[time_index][ue_index]['lstm_']
                        )
                        ue_RL_list[ue_index].do_store_reward(
                            episode,
                            time_index,
                            reward
                        )
                        ue_RL_list[ue_index].do_store_delay(
                            episode,
                            time_index,
                            env.process_delay[time_index, ue_index]
                        )
                        ue_RL_list[ue_index].do_store_energy(
                            episode,
                            time_index,
                            env.ue_comp_energy[time_index, ue_index],
                            env.ue_tran_energy [time_index, ue_index],
                            env.edge_comp_energy[time_index, ue_index],
                            env.ue_idle_energy[time_index, ue_index]
                        )
                        reward_indicator[time_index, ue_index] = 1
            """

            # ADD STEP (one step does not mean one store)
            RL_step += 1

            # UPDATE OBSERVATION
            observation_all = observation_all_
            lstm_state_all = lstm_state_all_

            # CONTROL LEARNING START TIME AND FREQUENCY
            if (RL_step > 200) and (RL_step % 10 == 0):
                for ue in range(env.n_ue):
                    ue_RL_list[ue].learn()

            # GAME ENDS

            if done:
                with open("Delay.txt", "a") as f:
                    f.write("\n" + str(Cal_Delay(ue_RL_list, episode)))

                with open("Energy.txt", "a") as f:
                    f.write("\n" + str(Cal_Energy(ue_RL_list, episode)))

                with open("QoE.txt", "a") as f:
                    f.write("\n" + str(Cal_QoE(ue_RL_list, episode)))

                with open("Drop.txt", "a") as f:
                    f.write("\n" + str(Drop_Count(ue_RL_list, episode)))

                """

                for task in env.task_history:
                    cmpl = drp = 0
                    for t in task:
                        d_states = t['d_state']
                        if any(d < 0 for d in d_states):
                            t['state'] = 'D'
                            drp += 1
                        elif all(d > 0 for d in d_states):
                            t['state'] = 'C'
                            cmpl += 1
                full_complete_task = 0
                full_drop_task = 0
                complete_task = 0
                drop_task = 0
                for history in env.task_history:
                    for task in history:
                        if task['state'] == 'C':
                            full_complete_task += 1
                        elif task['state'] == 'D':
                            full_drop_task += 1
                        for component_state in task['d_state']:
                            if component_state == 1:
                                complete_task += 1
                            elif component_state == -1:
                                drop_task += 1
                cnt = len(env.task_history) * len(env.task_history[0]) * env.n_component

                #a = Drop_Count(ue_RL_list, episode)
                """

                if episode % 200 == 0 and episode != 0:
                    os.mkdir("models" + "/" + str(episode))
                    for ue in range(env.n_ue):
                        ue_RL_list[ue].saver.save(
                            ue_RL_list[ue].sess,
                            "models/"
                            + str(episode)
                            + "/"
                            + str(ue)
                            + "_X_model"
                            + "/model.ckpt",
                            global_step=episode,
                        )
                        print("UE", ue, "Network_model_seved\n")

                if episode % 999 == 0 and episode != 0:
                    os.mkdir("models" + "/" + str(episode))
                    for ue in range(env.n_ue):
                        ue_RL_list[ue].saver.save(
                            ue_RL_list[ue].sess,
                            "models/"
                            + str(episode)
                            + "/"
                            + str(ue)
                            + "_X_model"
                            + "/model.ckpt",
                            global_step=episode,
                        )
                        print("UE", ue, "Network_model_seved\n")

                # Process energy
                ue_bit_processed = sum(sum(env.ue_bit_processed))
                ue_comp_energy = sum(sum(env.ue_comp_energy))

                # Transmission energy
                ue_bit_transmitted = sum(sum(env.ue_bit_transmitted))
                ue_tran_energy = sum(sum(env.ue_tran_energy))

                # edge energy
                edge_bit_processed = sum(sum(env.edge_bit_processed))
                edge_comp_energy = sum(sum(env.edge_comp_energy))
                ue_idle_energy = sum(sum(env.ue_idle_energy))

                avg_delay = Cal_Delay(ue_RL_list, episode)
                avg_energy = Cal_Energy(ue_RL_list, episode)
                avg_QoE = Cal_QoE(ue_RL_list, episode)

                avg_QoE_list.append(avg_QoE)
                avg_delay_list.append(avg_delay)
                energy_cons_list.append(avg_energy)
                num_drop_list.append(
                    env.drop_trans_count + env.drop_edge_count + env.drop_ue_count
                )

                avg_reward_list.append(-(Cal_QoE(ue_RL_list, episode)))

                # Append metrics to tracking lists
                if episode % 10 == 0:
                    avg_reward_list_2.append(
                        sum(avg_reward_list[episode - 10 : episode]) / 10
                    )
                    avg_delay_list_in_episode.append(Cal_Delay(ue_RL_list, episode))
                    avg_energy_list_in_episode.append(Cal_Energy(ue_RL_list, episode))

                    # Create a figure with 4 vertically stacked subplots
                    fig, axs = plt.subplots(4, 1, figsize=(10, 20))
                    fig.suptitle(
                        "Performance Metrics Over Episodes", fontsize=16, y=0.92
                    )

                    # Subplot for Average QoE
                    axs[0].plot(
                        avg_QoE_list,
                        marker="o",
                        linestyle="-",
                        color="b",
                        label="Avg QoE",
                    )
                    axs[0].set_title("", fontsize=14)
                    axs[0].set_ylabel("Average QoE")
                    axs[0].set_xlabel("Episode")
                    axs[0].grid(True, linestyle="--", alpha=0.7)
                    axs[0].legend()

                    # Subplot for Average Delay
                    axs[1].plot(
                        avg_delay_list,
                        marker="s",
                        linestyle="-",
                        color="g",
                        label="Avg Delay",
                    )
                    axs[1].set_title("", fontsize=14)
                    axs[1].set_ylabel("Average Delay")
                    axs[1].set_xlabel("Episode")
                    axs[1].grid(True, linestyle="--", alpha=0.7)
                    axs[1].legend()

                    # Subplot for Energy Consumption
                    axs[2].plot(
                        energy_cons_list,
                        marker="^",
                        linestyle="-",
                        color="r",
                        label="Energy Cons.",
                    )
                    axs[2].set_title("", fontsize=14)
                    axs[2].set_ylabel("Energy Consumption")
                    axs[2].set_xlabel("Episode")
                    axs[2].grid(True, linestyle="--", alpha=0.7)
                    axs[2].legend()

                    # Subplot for Number of Drops
                    axs[3].plot(
                        num_drop_list,
                        marker="x",
                        linestyle="-",
                        color="m",
                        label="Num Drops",
                    )
                    axs[3].set_title("", fontsize=14)
                    axs[3].set_ylabel("Number Drops")
                    axs[3].set_xlabel("Episode")
                    axs[3].grid(True, linestyle="--", alpha=0.7)
                    axs[3].legend()

                    # Save the figure to a file
                    plt.tight_layout()
                    plt.subplots_adjust(top=0.9)
                    plt.savefig("Performance_Chart.png", dpi=100)
                    # plt.show()

                print(
                    "SystemPerformance: ---------------------------------------------------------------------"
                )
                # print("Num_Completed :  ", )
                print(
                    "Num_Dropped   :  ",
                    env.drop_trans_count + env.drop_edge_count + env.drop_ue_count,
                    "[Trans_Drop: ",
                    env.drop_trans_count,
                    "Edge_Drop: ",
                    env.drop_edge_count,
                    "UE_Drop: ",
                    env.drop_ue_count,
                    "]",
                )
                print("Avg_Delay     :  ", "%0.1f" % avg_delay)
                print("Avg_Energy    :  ", "%0.1f" % avg_energy)
                print("Avg_QoE       :  ", "%0.1f" % avg_QoE)
                print(
                    "EnergyCosumption: ----------------------------------------------------------------------"
                )
                print(
                    "Local         :  ",
                    "%0.1f" % ue_comp_energy,
                    "[ue_bit_processed:",
                    int(ue_bit_processed),
                    "]",
                )
                print(
                    "Trans         :  ",
                    "%0.1f" % ue_tran_energy,
                    "[ue_bit_transmitted:",
                    int(ue_bit_transmitted),
                    "]",
                )
                print(
                    "Edges         :  ",
                    "%0.1f" % sum(ue_idle_energy),
                    "[edge_bit_processed :",
                    int(sum(edge_bit_processed)),
                    "]",
                )
                # print("--------------------------------------------------------------------------------------------------------")
                # print("Trans_Drop: ", env.drop_trans_count, "Edge_Drop: ", env.drop_edge_count, "UE_Drop: ", env.drop_ue_count)
                # print("Drop_Count: ",Drop_Count(ue_RL_list, episode))

                break  # Training Finished


if __name__ == "__main__":

    # GENERATE ENVIRONMENT
    env = MEC(
        Config.N_UE, Config.N_EDGE, Config.N_TIME, Config.N_COMPONENT, Config.MAX_DELAY
    )

    # GENERATE MULTIPLE CLASSES FOR RL
    ue_RL_list = list()
    for ue in range(Config.N_UE):
        ue_RL_list.append(
            DuelingDoubleDeepQNetwork(
                env.n_actions,
                env.n_features,
                env.n_lstm_state,
                env.n_time,
                learning_rate=Config.LEARNING_RATE,
                reward_decay=Config.REWARD_DECAY,
                e_greedy=Config.E_GREEDY,
                replace_target_iter=Config.N_NETWORK_UPDATE,
                memory_size=Config.MEMORY_SIZE,
            )
        )

    # LOAD Trained MODEL
    """
    for ue in range(Config.N_UE):
        ue_RL_list[ue].Initialize(ue_RL_list[ue].sess, ue)
        ue_RL_list[ue].epsilon = 1
    """

    Delay = open("Delay.txt", "w")
    Energy = open("Energy.txt", "w")
    QoE = open("QoE.txt", "w")
    Drop = open("Drop.txt", "w")

    # TRAIN THE SYSTEM
    train(ue_RL_list, Config.N_EPISODE)
MEC_Env.py :
from Config import Config
import numpy as np
import random
import math
import queue

class MEC:
    def __init__(self, num_ue, num_edge, num_time, num_component, max_delay):
        # Initialize variables
        self.n_ue          = num_ue
        self.n_edge        = num_edge
        self.n_time        = num_time
        self.n_component   = num_component
        self.max_delay     = max_delay
        self.duration      = Config.DURATION
        self.ue_p_comp     = Config.UE_COMP_ENERGY
        self.ue_p_tran     = Config.UE_TRAN_ENERGY
        self.ue_p_idle     = Config.UE_IDLE_ENERGY
        self.edge_p_comp   = Config.EDGE_COMP_ENERGY
        

        self.time_count      = 0
        self.task_count_ue   = 0
        self.task_count_edge = 0
        self.n_actions       = 1 + self.n_edge
        self.n_features      = 1 + 1 + 1 + 1 + self.n_edge
        self.n_lstm_state    = self.n_edge

        self.drop_trans_count = 0
        self.drop_edge_count = 0
        self.drop_ue_count = 0
  

        # Computation and transmission capacities
        self.comp_cap_ue   = Config.UE_COMP_CAP * np.ones(self.n_ue) * self.duration
        self.comp_cap_edge = Config.EDGE_COMP_CAP * np.ones([self.n_edge]) * self.duration
        self.tran_cap_ue   = Config.UE_TRAN_CAP * np.ones([self.n_ue, self.n_edge]) * self.duration
        self.n_cycle = 1
        self.task_arrive_prob = Config.TASK_ARRIVE_PROB
        self.max_arrive_size   = Config.TASK_MAX_SIZE
        self.min_arrive_size   = Config.TASK_MIN_SIZE
        self.arrive_task_size_set    = np.arange(self.min_arrive_size, self.max_arrive_size, 0.1)
        #self.energy_state_set   = np.arange(0.25,1, 0.25) 
        self.ue_energy_state = [Config.UE_ENERGY_STATE[np.random.randint(0,len(Config.UE_ENERGY_STATE))] for ue in range(self.n_ue)]
        self.arrive_task_size   = np.zeros([self.n_time, self.n_ue])
        self.arrive_task_dens   = np.zeros([self.n_time, self.n_ue])




        #print(self.energy_state_set)

        #print(self.ue_energy_state)


        #self.comp_density=0.297



        self.n_task = int(self.n_time * self.task_arrive_prob)

        # Task delay and energy-related arrays
        self.process_delay = np.zeros([self.n_time, self.n_ue])
        self.ue_bit_processed = np.zeros([self.n_time, self.n_ue])
        self.edge_bit_processed = np.zeros([self.n_time, self.n_ue, self.n_edge])
        self.ue_bit_transmitted = np.zeros([self.n_time, self.n_ue])
        self.ue_comp_energy = np.zeros([self.n_time, self.n_ue])
        self.edge_comp_energy = np.zeros([self.n_time, self.n_ue, self.n_edge])
        self.ue_idle_energy = np.zeros([self.n_time, self.n_ue, self.n_edge])
        self.ue_tran_energy = np.zeros([self.n_time, self.n_ue])
        self.unfinish_task = np.zeros([self.n_time, self.n_ue])
        self.process_delay_trans = np.zeros([self.n_time, self.n_ue])
        self.edge_drop = np.zeros([self.n_ue, self.n_edge])

        # Queue information initialization
        self.t_ue_comp = -np.ones([self.n_ue])
        self.t_ue_tran = -np.ones([self.n_ue])
        self.b_edge_comp = np.zeros([self.n_ue, self.n_edge])

        # Queue initialization
        self.ue_computation_queue = [queue.Queue() for _ in range(self.n_ue)]
        self.ue_transmission_queue = [queue.Queue() for _ in range(self.n_ue)]
        self.edge_computation_queue = [[queue.Queue() for _ in range(self.n_edge)] for _ in range(self.n_ue)]
        self.edge_ue_m = np.zeros(self.n_edge)
        self.edge_ue_m_observe = np.zeros(self.n_edge)

        # Task indicator initialization
        self.local_process_task = [{'DIV': np.nan, 'UE_ID': np.nan, 'TASK_ID': np.nan, 'SIZE': np.nan,
                                    'TIME': np.nan, 'EDGE': np.nan, 'REMAIN': np.nan} for _ in range(self.n_ue)]
        self.local_transmit_task = [{'DIV': np.nan, 'UE_ID': np.nan, 'TASK_ID': np.nan, 'SIZE': np.nan,
                                     'TIME': np.nan, 'EDGE': np.nan, 'REMAIN': np.nan} for _ in range(self.n_ue)]
        self.edge_process_task = [[{'DIV': np.nan, 'UE_ID': np.nan, 'TASK_ID': np.nan, 'SIZE': np.nan,
                                    'TIME': np.nan, 'REMAIN': np.nan} for _ in range(self.n_edge)] for _ in range(self.n_ue)]

        self.task_history = [[] for _ in range(self.n_ue)]

    def reset(self, arrive_task_size, arrive_task_dens):
    
        self.drop_trans_count = 0
        self.drop_edge_count = 0
        self.drop_ue_count = 0

        # Reset variables and queues
        self.task_history = [[] for _ in range(self.n_ue)]
        self.UE_TASK = [-1] * self.n_ue
        self.drop_edge_count = 0

        self.arrive_task_size = arrive_task_size
        self.arrive_task_dens = arrive_task_dens

        self.time_count = 0

        self.local_process_task = []
        self.local_transmit_task = []
        self.edge_process_task = []



        self.ue_computation_queue = [queue.Queue() for _ in range(self.n_ue)]
        self.ue_transmission_queue = [queue.Queue() for _ in range(self.n_ue)]
        self.edge_computation_queue = [[queue.Queue() for _ in range(self.n_edge)] for _ in range(self.n_ue)]
        
        self.t_ue_comp = -np.ones([self.n_ue])
        self.t_ue_tran = -np.ones([self.n_ue])
        self.b_edge_comp = np.zeros([self.n_ue, self.n_edge])

        self.process_delay = np.zeros([self.n_time, self.n_ue])
        self.ue_bit_processed = np.zeros([self.n_time, self.n_ue])
        self.edge_bit_processed = np.zeros([self.n_time, self.n_ue, self.n_edge])
        self.ue_bit_transmitted = np.zeros([self.n_time, self.n_ue])
        self.ue_comp_energy = np.zeros([self.n_time, self.n_ue])
        self.edge_comp_energy = np.zeros([self.n_time, self.n_ue, self.n_edge])
        self.ue_idle_energy = np.zeros([self.n_time, self.n_ue, self.n_edge])
        self.ue_tran_energy = np.zeros([self.n_time, self.n_ue])
        self.unfinish_task = np.zeros([self.n_time, self.n_ue])
        self.process_delay_trans = np.zeros([self.n_time, self.n_ue])
        self.edge_drop = np.zeros([self.n_ue, self.n_edge])

        self.local_process_task = [{'DIV': np.nan, 'UE_ID': np.nan, 'TASK_ID': np.nan, 'SIZE': np.nan,
                                    'TIME': np.nan, 'EDGE': np.nan, 'REMAIN': np.nan} for _ in range(self.n_ue)]
        self.local_transmit_task = [{'DIV': np.nan, 'UE_ID': np.nan, 'TASK_ID': np.nan, 'SIZE': np.nan,
                                     'TIME': np.nan, 'EDGE': np.nan, 'REMAIN': np.nan} for _ in range(self.n_ue)]
        self.edge_process_task = [[{'DIV': np.nan, 'UE_ID': np.nan, 'TASK_ID': np.nan, 'SIZE': np.nan,
                                    'TIME': np.nan, 'REMAIN': np.nan} for _ in range(self.n_edge)] for _ in range(self.n_ue)]

        # Initial observation and LSTM state
        UEs_OBS = np.zeros([self.n_ue, self.n_features])
        for ue_index in range(self.n_ue):
            if self.arrive_task_size[self.time_count, ue_index] != 0:
                UEs_OBS[ue_index, :] = np.hstack([
                    self.arrive_task_size[self.time_count, ue_index], self.t_ue_comp[ue_index],
                    self.t_ue_tran[ue_index],
                    np.squeeze(self.b_edge_comp[ue_index, :]),
                    self.ue_energy_state[ue_index]])

        UEs_lstm_state = np.zeros([self.n_ue, self.n_lstm_state])

        return UEs_OBS, UEs_lstm_state

   
    # perform action, observe state and delay (several steps later)
    def step(self, action):
    

        ue_action_local = np.zeros([self.n_ue], np.int32)
        ue_action_offload = np.zeros([self.n_ue], np.int32)

        for ue_index in range(self.n_ue):
            ue_action = action[ue_index]
            ue_action_offload[ue_index] = int(ue_action - 1)
            if ue_action == 0:
                ue_action_local[ue_index] = 1


        #ue_action_offload = np.zeros([self.n_ue], np.int32)
        #ue_action_component = np.zeros([self.n_ue], np.int32)-1
        #random_list  = []
        #for i in range(self.n_component):
            #random_list.append(i)

        # COMPUTATION QUEUE UPDATE ===================
        for ue_index in range(self.n_ue):

            ue_comp_cap = np.squeeze(self.comp_cap_ue[ue_index])
            ue_arrive_task_size = np.squeeze(self.arrive_task_size[self.time_count, ue_index])
            ue_arrive_task_dens = np.squeeze(self.arrive_task_dens[self.time_count, ue_index])
        
            tmp_dict = {
                'DIV' : 0 , 
                'UE_ID': ue_index,
                'TASK_ID': self.UE_TASK[ue_index],
                'SIZE': ue_arrive_task_size,
                'DENS': ue_arrive_task_dens,
                'TIME': self.time_count,
                'EDGE': ue_action_offload[ue_index],
            }

            if ue_action_local[ue_index] == 1:
                self.ue_computation_queue[ue_index].put(tmp_dict)




            '''
            component_list = np.zeros([self.n_component], np.int32)-1
            state_list = np.zeros([self.n_component], np.int32)
            ue_action = action[ue_index]
            if ue_action == 0:
                ue_action_local[ue_index] = 1
            else:
                ue_action_offload[ue_index] = 1
                sample = random.sample(random_list, int(ue_action))
                for i in range(len(sample)):
                    component_list[sample[i]] = np.random.randint(0, self.n_edge)

            ue_action_component[ue_index] = action[ue_index]
            ue_comp_cap = np.squeeze(self.comp_cap_ue[ue_index])
            ue_arrive_task_size = np.squeeze(self.arrive_task_size[self.time_count, ue_index])
            ue_arrive_task_dens = np.squeeze(self.arrive_task_dens[self.time_count, ue_index])

            if ue_arrive_task_size > 0:
                self.UE_TASK[ue_index] += 1
                task_dic = {
                    'UE_ID': ue_index,
                    'TASK_ID': self.UE_TASK[ue_index],
                    'SIZE': ue_arrive_task_size,
                    'DENS': ue_arrive_task_dens,
                    'TIME': self.time_count,
                    'EDGE': component_list,
                    'd_state': state_list,
                    'state': np.nan
                }
                self.task_history[ue_index].append(task_dic) 

            
            for component in range(self.n_component):
                temp_dic = {
                    'DIV': component,
                    'UE_ID': ue_index,
                    'TASK_ID': self.UE_TASK[ue_index],
                    'SIZE': ue_arrive_task_size / self.n_component,
                    'DENS': ue_arrive_task_dens,
                    'TIME': self.time_count,
                    'EDGE': component_list[component],
                    'd_state': state_list[component]
                }
                #if component_list[component] > -1:
                    #self.ue_transmission_queue[ue_index].put(temp_dic) 
                #else:
                    #self.ue_computation_queue[ue_index].put(temp_dic)
            '''


            for cycle in range(self.n_cycle):    
                # TASK ON PROCESS
                if math.isnan(self.local_process_task[ue_index]['REMAIN']) \
                        and (not self.ue_computation_queue[ue_index].empty()):
                    while not self.ue_computation_queue[ue_index].empty():
                        get_task = self.ue_computation_queue[ue_index].get()
                        #print(get_task)
                        if get_task['SIZE'] != 0:
                            if self.time_count - get_task['TIME'] + 1 <= self.max_delay:
                                self.local_process_task[ue_index]['UE_ID']    = get_task['UE_ID']
                                self.local_process_task[ue_index]['TASK_ID']  = get_task['TASK_ID']
                                self.local_process_task[ue_index]['SIZE']     = get_task['SIZE']
                                self.local_process_task[ue_index]['DENS']     = get_task['DENS']
                                self.local_process_task[ue_index]['TIME']     = get_task['TIME']
                                self.local_process_task[ue_index]['REMAIN']   = self.local_process_task[ue_index]['SIZE']
                                self.local_process_task[ue_index]['DIV']      = get_task['DIV']

                                break
                            else:
                                #self.task_history[ue_index][get_task['TASK_ID']]['d_state'][get_task['DIV']] = -1
                                
                                self.process_delay[get_task['TIME'], ue_index] = self.max_delay
                                self.unfinish_task[get_task['TIME'], ue_index] = 1

                             
                # PROCESS
                if self.local_process_task[ue_index]['REMAIN'] > 0:

                    if self.local_process_task[ue_index]['REMAIN'] >= (ue_comp_cap / self.local_process_task[ue_index]['DENS']):
    
                        self.ue_bit_processed[self.local_process_task[ue_index]['TIME'], ue_index] += ue_comp_cap / self.local_process_task[ue_index]['DENS']
                        self.ue_comp_energy[self.local_process_task[ue_index]['TIME'], ue_index] += (ue_comp_cap / self.local_process_task[ue_index]['DENS']) * (1 ** (-27) * (ue_comp_cap / self.local_process_task[ue_index]['DENS'])) 
                    else:
                        self.ue_bit_processed[self.local_process_task[ue_index]['TIME'], ue_index] += self.local_process_task[ue_index]['REMAIN']/ self.local_process_task[ue_index]['DENS']
                        self.ue_comp_energy[self.local_process_task[ue_index]['TIME'], ue_index] += self.local_process_task[ue_index]['REMAIN']/ self.local_process_task[ue_index]['DENS'] * (1 ** (-27) * (ue_comp_cap / self.local_process_task[ue_index]['DENS']))


                    self.local_process_task[ue_index]['REMAIN'] = \
                        self.local_process_task[ue_index]['REMAIN'] - ue_comp_cap / self.local_process_task[ue_index]['DENS']

                    #print(self.local_process_task[ue_index]['REMAIN'])
                    #print(ue_comp_cap, self.local_process_task[ue_index]['DENS'])

                    # if no remain, compute processing delay
                    if self.local_process_task[ue_index]['REMAIN'] <= 0: 
                        self.process_delay[self.local_process_task[ue_index]['TIME'], ue_index] \
                            = self.time_count - self.local_process_task[ue_index]['TIME'] + 1
                        self.local_process_task[ue_index]['REMAIN'] = np.nan
                        #print("hi")

                        #self.task_history[ue_index][self.local_process_task[ue_index]['TASK_ID']]['d_state'][self.local_process_task[ue_index]['DIV']] = 1 
                        #if sum(self.task_history[ue_index][self.local_process_task[ue_index]['TASK_ID']]['d_state']) > self.n_component-1:


                    elif self.time_count - self.local_process_task[ue_index]['TIME'] + 1 == self.max_delay:
                        #self.task_history[ue_index][self.local_process_task[ue_index]['TASK_ID']]['d_state'][self.local_process_task[ue_index]['DIV']] = -1
                        self.local_process_task[ue_index]['REMAIN'] = np.nan
                        self.process_delay[self.local_process_task[ue_index]['TIME'], ue_index] = self.max_delay
                        self.unfinish_task[self.local_process_task[ue_index]['TIME'], ue_index] = 1
                        self.drop_ue_count = self.drop_ue_count + 1

                    # OTHER INFO self.t_ue_comp[ue_index]
                    # update self.t_ue_comp[ue_index] only when ue_bitrate != 0
                if ue_arrive_task_size != 0:
                    tmp_tilde_t_ue_comp = np.max([self.t_ue_comp[ue_index] + 1, self.time_count])
                    self.t_ue_comp[ue_index] = np.min([tmp_tilde_t_ue_comp
                                                                    + math.ceil(ue_arrive_task_size * ue_action_local[ue_index]
                                                                    / (ue_comp_cap / ue_arrive_task_dens)) - 1,
                                                                    self.time_count + self.max_delay - 1])

        # edge QUEUE UPDATE =========================
        for ue_index in range(self.n_ue):
            #ue_comp_density = self.comp_density

            for edge_index in range(self.n_edge):
                edge_cap = self.comp_cap_edge[edge_index]/self.n_cycle
  
                for cycle in range(self.n_cycle): 
                    # TASK ON PROCESS
                    if math.isnan(self.edge_process_task[ue_index][edge_index]['REMAIN']) \
                            and (not self.edge_computation_queue[ue_index][edge_index].empty()):
                        while not self.edge_computation_queue[ue_index][edge_index].empty():
                            get_task = self.edge_computation_queue[ue_index][edge_index].get()

                                            

                            if self.time_count - get_task['TIME'] + 1 <= self.max_delay:
                                self.edge_process_task[ue_index][edge_index]['UE_ID']   = get_task['UE_ID']
                                self.edge_process_task[ue_index][edge_index]['TASK_ID'] = get_task['TASK_ID']
                                self.edge_process_task[ue_index][edge_index]['SIZE']    = get_task['SIZE']
                                self.edge_process_task[ue_index][edge_index]['DENS']    = get_task['DENS']
                                self.edge_process_task[ue_index][edge_index]['TIME']    = get_task['TIME']
                                self.edge_process_task[ue_index][edge_index]['REMAIN']  = self.edge_process_task[ue_index][edge_index]['SIZE']
                                self.edge_process_task[ue_index][edge_index]['DIV']     = get_task['DIV']
                                break
                            else:
                                
                                #self.task_history[get_task['UE_ID']][get_task['TASK_ID']]['d_state'][get_task['DIV']] = -1
                                self.process_delay[get_task['TIME'], ue_index] = self.max_delay
                                self.unfinish_task[get_task['TIME'], ue_index] = 1


                #    print(self.edge_process_task[ue_index][edge_index], "f_________")
                    # PROCESS
                    self.edge_drop[ue_index, edge_index] = 0

                    if self.edge_process_task[ue_index][edge_index]['REMAIN'] > 0:
    
                        if self.edge_process_task[ue_index][edge_index]['REMAIN'] >= (edge_cap / self.edge_process_task[ue_index][edge_index]['DENS'] / self.edge_ue_m[edge_index]):
                            self.edge_comp_energy[self.edge_process_task[ue_index][edge_index]['TIME'], ue_index, edge_index] += (edge_cap/ self.edge_process_task[ue_index][edge_index]['DENS']) * (self.edge_p_comp * self.duration)
                            self.edge_bit_processed[self.edge_process_task[ue_index][edge_index]['TIME'], ue_index, edge_index] += (edge_cap/ self.edge_process_task[ue_index][edge_index]['DENS'] / self.edge_ue_m[edge_index])                      
                            self.ue_idle_energy[self.edge_process_task[ue_index][edge_index]['TIME'], ue_index, edge_index] += (edge_cap / self.edge_process_task[ue_index][edge_index]['DENS'] / self.edge_ue_m[edge_index]) 
                        else:
                            self.edge_bit_processed[self.edge_process_task[ue_index][edge_index]['TIME'], ue_index, edge_index] += self.edge_process_task[ue_index][edge_index]['REMAIN'] / self.edge_ue_m[edge_index]
                            self.edge_comp_energy[self.edge_process_task[ue_index][edge_index]['TIME'], ue_index, edge_index] += (self.edge_process_task[ue_index][edge_index]['REMAIN']) * (self.edge_p_comp * self.duration)
                            self.ue_idle_energy[self.edge_process_task[ue_index][edge_index]['TIME'], ue_index, edge_index] += (self.edge_process_task[ue_index][edge_index]['REMAIN'] / self.edge_ue_m[edge_index]) * self.ue_p_idle  

                        self.edge_process_task[ue_index][edge_index]['REMAIN'] = self.edge_process_task[ue_index][edge_index]['REMAIN'] - edge_cap/ self.edge_process_task[ue_index][edge_index]['DENS'] / self.edge_ue_m[edge_index]
                        
                        

                        if self.edge_process_task[ue_index][edge_index]['REMAIN'] <= 0:
                            self.process_delay[self.edge_process_task[ue_index][edge_index]['TIME'],ue_index] \
                                = self.time_count - self.edge_process_task[ue_index][edge_index]['TIME'] + 1

                            #self.task_history[self.edge_process_task[ue_index][edge_index]['UE_ID']][self.edge_process_task[ue_index][edge_index]['TASK_ID']]['d_state'][self.edge_process_task[ue_index][edge_index]['DIV']] = 1
                            self.edge_process_task[ue_index][edge_index]['REMAIN'] = np.nan
                            '''
                            if sum(self.task_history[ue_index][self.edge_process_task[ue_index][edge_index]['TASK_ID']]['d_state']) > self.n_component-1:
                                self.process_delay[self.edge_process_task[ue_index][edge_index]['TIME'],ue_index] \
                                    = self.time_count - self.edge_process_task[ue_index][edge_index]['TIME'] + 1
                            '''


                        elif self.time_count - self.edge_process_task[ue_index][edge_index]['TIME'] + 1 == self.max_delay:
                            #self.task_history[self.edge_process_task[ue_index][edge_index]['UE_ID']][self.edge_process_task[ue_index][edge_index]['TASK_ID']]['d_state'][self.edge_process_task[ue_index][edge_index]['DIV']] = -1
                            self.edge_drop[ue_index, edge_index] = self.edge_process_task[ue_index][edge_index]['REMAIN']
                            self.process_delay[self.edge_process_task[ue_index][edge_index]['TIME'], ue_index] = self.max_delay
                            self.unfinish_task[self.edge_process_task[ue_index][edge_index]['TIME'], ue_index] = 1
                            self.edge_process_task[ue_index][edge_index]['REMAIN'] = np.nan
                            self.drop_edge_count = self.drop_edge_count + 1


                        #self.TASK_log[ue_index][self.edge_process_task[ue_index][edge_index]['TASK_ID']]['state'] = 2

                # OTHER INFO
                    if self.edge_ue_m[edge_index] != 0:
                        self.b_edge_comp[ue_index, edge_index] \
                            = np.max([self.b_edge_comp[ue_index, edge_index]
                                        - self.comp_cap_edge[edge_index]/ ue_arrive_task_dens / self.edge_ue_m[edge_index]
                                        - self.edge_drop[ue_index, edge_index], 0])

        # TRANSMISSION QUEUE UPDATE ===================
        for ue_index in range(self.n_ue):
            #ue_tran_cap = np.squeeze(self.tran_cap_ue[ue_index,:])[1]/self.n_cycle

            ue_tran_cap = np.squeeze(self.tran_cap_ue[ue_index,:])
            ue_arrive_task_size = np.squeeze(self.arrive_task_size[self.time_count, ue_index])
            ue_arrive_task_dens = np.squeeze(self.arrive_task_dens[self.time_count, ue_index])
        
            tmp_dict = {
                'DIV' : 0 , 
                'UE_ID': ue_index,
                'TASK_ID': self.UE_TASK[ue_index],
                'SIZE': ue_arrive_task_size,
                'DENS': ue_arrive_task_dens,
                'TIME': self.time_count,
                'EDGE': ue_action_offload[ue_index],
            }


            if ue_action_local[ue_index] == 0:
                self.ue_transmission_queue[ue_index].put(tmp_dict)

          


            
            for cycle in range(self.n_cycle):

                # TASK ON PROCESS
                if math.isnan(self.local_transmit_task[ue_index]['REMAIN']) \
                        and (not self.ue_transmission_queue[ue_index].empty()):
                    while not self.ue_transmission_queue[ue_index].empty():
                        get_task = self.ue_transmission_queue[ue_index].get()
                        #print("trans", get_task)
                        if get_task['SIZE'] != 0:
                            
                            #self.TASK_log[ue_index][get_task['TASK_ID']] = get_task
                            #self.task_history[ue_index].append(get_task)

                            if self.time_count - get_task['TIME'] + 1 <= self.max_delay:
                                self.local_transmit_task[ue_index]['UE_ID'] = get_task['UE_ID']
                                self.local_transmit_task[ue_index]['TASK_ID'] = get_task['TASK_ID']
                                self.local_transmit_task[ue_index]['SIZE'] = get_task['SIZE']
                                self.local_transmit_task[ue_index]['DENS'] = get_task['DENS']
                                self.local_transmit_task[ue_index]['TIME'] = get_task['TIME']
                                self.local_transmit_task[ue_index]['EDGE'] = int(get_task['EDGE'])
                                self.local_transmit_task[ue_index]['REMAIN'] = self.local_transmit_task[ue_index]['SIZE']
                                self.local_transmit_task[ue_index]['DIV'] = get_task['DIV']
                                break
                            else:
                                #self.task_history[get_task['UE_ID']][get_task['TASK_ID']]['d_state'][get_task['DIV']] = -1
                                self.process_delay[get_task['TIME'], ue_index] = self.max_delay
                                self.unfinish_task[get_task['TIME'], ue_index] = 1


                # PROCESS
                if self.local_transmit_task[ue_index]['REMAIN'] > 0:

                    if self.local_transmit_task[ue_index]['REMAIN'] >= ue_tran_cap[self.local_transmit_task[ue_index]['EDGE']]:
                        self.ue_tran_energy[self.local_transmit_task[ue_index]['TIME'], ue_index] += ue_tran_cap[self.local_transmit_task[ue_index]['EDGE']] * self.ue_p_tran
                        self.ue_bit_transmitted[self.local_transmit_task[ue_index]['TIME'], ue_index] += self.local_transmit_task[ue_index]['REMAIN'] 
                    
                    else:
                        self.ue_tran_energy[self.local_transmit_task[ue_index]['TIME'], ue_index] += ue_tran_cap[self.local_transmit_task[ue_index]['EDGE']] * self.ue_p_tran
                        self.ue_bit_transmitted[self.local_transmit_task[ue_index]['TIME'], ue_index] += self.local_transmit_task[ue_index]['REMAIN'] 

                    self.local_transmit_task[ue_index]['REMAIN'] = \
                        self.local_transmit_task[ue_index]['REMAIN'] \
                        - ue_tran_cap[self.local_transmit_task[ue_index]['EDGE']]

                    #print(ue_tran_cap)

                    # UPDATE edge QUEUE
                    if self.local_transmit_task[ue_index]['REMAIN'] <= 0:
                        tmp_dict = {'UE_ID': self.local_transmit_task[ue_index]['UE_ID'],
                                    'TASK_ID': self.local_transmit_task[ue_index]['TASK_ID'],
                                    'SIZE' : self.local_transmit_task[ue_index]['SIZE'],
                                    'DENS' : self.local_transmit_task[ue_index]['DENS'],
                                    'TIME' : self.local_transmit_task[ue_index]['TIME'],
                                    'EDGE'  : self.local_transmit_task[ue_index]['EDGE'],
                                    'DIV'  : self.local_transmit_task[ue_index]['DIV']}

        
                        self.edge_computation_queue[ue_index][self.local_transmit_task[ue_index]['EDGE']].put(tmp_dict)
                        #print("_+_+____", self.local_transmit_task[ue_index]['EDGE'])
                        self.task_count_edge = self.task_count_edge + 1

                        edge_index = self.local_transmit_task[ue_index]['EDGE']
                        self.b_edge_comp[ue_index, edge_index] = self.b_edge_comp[ue_index, edge_index] + self.local_transmit_task[ue_index]['SIZE']
                        
                        self.process_delay_trans[self.local_transmit_task[ue_index]['TIME'], ue_index] = self.time_count - self.local_transmit_task[ue_index]['TIME'] + 1
                        self.local_transmit_task[ue_index]['REMAIN'] = np.nan


                    elif self.time_count - self.local_transmit_task[ue_index]['TIME'] + 1 == self.max_delay:
                        #self.task_history[self.local_transmit_task[ue_index]['UE_ID']][self.local_transmit_task[ue_index]['TASK_ID']]['d_state'][self.local_transmit_task[ue_index]['DIV']] = -1
                        self.local_transmit_task[ue_index]['REMAIN'] = np.nan
                        self.process_delay[self.local_transmit_task[ue_index]['TIME'], ue_index] = self.max_delay
                        self.unfinish_task[self.local_transmit_task[ue_index]['TIME'], ue_index] = 1
                        self.drop_trans_count = self.drop_trans_count + 1

                    # OTHER INFO
                if ue_arrive_task_size != 0:
                    tmp_tilde_t_ue_tran = np.max([self.t_ue_tran[ue_index] + 1, self.time_count])
                    self.t_ue_comp[ue_index] = np.min([tmp_tilde_t_ue_tran
                                                            + math.ceil(ue_arrive_task_size * (1 - ue_action_local[ue_index])
                                                            / ue_tran_cap[ue_action_offload[ue_index]]) - 1,
                                                            self.time_count + self.max_delay - 1])
            



        # COMPUTE CONGESTION (FOR NEXT TIME SLOT)
        self.edge_ue_m_observe = self.edge_ue_m
        self.edge_ue_m = np.zeros(self.n_edge)
        for edge_index in range(self.n_edge):
            for ue_index in range(self.n_ue):
                if (not self.edge_computation_queue[ue_index][edge_index].empty()) \
                        or self.edge_process_task[ue_index][edge_index]['REMAIN'] > 0:
                    self.edge_ue_m[edge_index] += 1


        # TIME UPDATE
        self.time_count = self.time_count + 1
        done = False
        if self.time_count >= self.n_time:
            done = True
            # set all the tasks' processing delay and unfinished indicator
            
            for time_index in range(self.n_time):
                for ue_index in range(self.n_ue):
                    if self.process_delay[time_index, ue_index] == 0 and self.arrive_task_size[time_index, ue_index] != 0:
                        self.process_delay[time_index, ue_index] = (self.time_count - 1) - time_index + 1
                        self.unfinish_task[time_index, ue_index] = 1


        # OBSERVATION
        UEs_OBS_ = np.zeros([self.n_ue, self.n_features])
        UEs_lstm_state_ = np.zeros([self.n_ue, self.n_lstm_state])
        if not done:
            for ue_index in range(self.n_ue):
                # observation is zero if there is no task arrival
                if self.arrive_task_size[self.time_count, ue_index] != 0:
                    # state [A, B^{comp}, B^{tran}, [B^{edge}]]
                    UEs_OBS_[ue_index, :] = np.hstack([
                        self.arrive_task_size[self.time_count, ue_index],
                        self.t_ue_comp[ue_index] - self.time_count + 1,
                        self.t_ue_tran[ue_index] - self.time_count + 1,
                        self.b_edge_comp[ue_index, :],
                        self.ue_energy_state[ue_index]])

                UEs_lstm_state_[ue_index, :] = np.hstack(self.edge_ue_m_observe)

        return UEs_OBS_, UEs_lstm_state_, done


            
        '''
        for ue in range(self.n_ue):
            for task in range(len(self.task_history[ue])):
                #print(self.task_history[ue][task], "\n")
                for component in range(self.n_component):
                    
                    if self.task_history[ue][task]['d_state'][component] < 0:
                        self.process_delay[self.task_history[ue][task]['TIME'], ue] = self.max_delay
                        self.unfinish_task[self.task_history[ue][task]['TIME'], ue] = 1
        '''
   
D3QN.py
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
from collections import deque
from tensorflow.python.framework import ops
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior()
class DuelingDoubleDeepQNetwork:

    def __init__(self,
                 n_actions,               
                 n_features,
                 n_lstm_features,
                 n_time,
                 learning_rate = 0.01,
                 reward_decay = 0.9,
                 e_greedy = 0.99,
                 replace_target_iter = 200,  
                 memory_size = 500,  
                 batch_size=32,
                 e_greedy_increment= 0.00025,
                 n_lstm_step = 10,
                 dueling = True,
                 double_q = True,
                 N_L1 = 20,
                 N_lstm = 20):

        self.n_actions = n_actions
        self.n_features = n_features
        self.n_time = n_time
        self.lr = learning_rate
        self.gamma = reward_decay
        self.epsilon_max = e_greedy
        self.replace_target_iter = replace_target_iter
        self.memory_size = memory_size
        self.batch_size = batch_size    # select self.batch_size number of time sequence for learning
        self.epsilon_increment = e_greedy_increment
        self.epsilon = 0 if e_greedy_increment is not None else self.epsilon_max
        self.dueling = dueling
        self.double_q = double_q
        self.learn_step_counter = 0
        self.N_L1 = N_L1

        # lstm
        self.N_lstm = N_lstm
        self.n_lstm_step = n_lstm_step       # step_size in lstm
        self.n_lstm_state = n_lstm_features  # [fog1, fog2, ...., fogn, M_n(t)]

        # initialize zero memory np.hstack((s, [a, r], s_, lstm_s, lstm_s_))
        self.memory = np.zeros((self.memory_size, self.n_features + 1 + 1
                                    + self.n_features + self.n_lstm_state + self.n_lstm_state))

        # consist of [target_net, evaluate_net]
        self._build_net()

        # replace the parameters in target net
        t_params = tf.get_collection('target_net_params')  # obtain the parameters in target_net
        e_params = tf.get_collection('eval_net_params')  # obtain the parameters in eval_net
        self.replace_target_op = [tf.assign(t, e) for t, e in
                                      zip(t_params, e_params)]  # update the parameters in target_net

        self.sess = tf.Session()

        self.sess.run(tf.global_variables_initializer())
        self.reward_store = list()
        self.action_store = list()
        self.delay_store = list()
        self.energy_store = list()

        self.lstm_history = deque(maxlen=self.n_lstm_step)
        for ii in range(self.n_lstm_step):
            self.lstm_history.append(np.zeros([self.n_lstm_state]))

        self.store_q_value = list()

        self.saver = tf.train.Saver(var_list=tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES))

    def _build_net(self):

        tf.reset_default_graph()

        def build_layers(s,lstm_s,c_names, n_l1, n_lstm, w_initializer, b_initializer):

            # lstm for load levels
            with tf.variable_scope('l0'):
                lstm_dnn = tf.compat.v1.nn.rnn_cell.BasicLSTMCell(n_lstm)
                lstm_dnn.zero_state(self.batch_size, tf.float32)
                lstm_output,lstm_state = tf.nn.dynamic_rnn(lstm_dnn, lstm_s, dtype=tf.float32)
                lstm_output_reduced = tf.reshape(lstm_output[:, -1, :], shape=[-1, n_lstm])

            # first layer
            with tf.variable_scope('l1'):
                w1 = tf.get_variable('w1',[n_lstm + self.n_features, n_l1], initializer=w_initializer,
                                     collections=c_names)
                b1 = tf.get_variable('b1',[1,n_l1],initializer=b_initializer, collections=c_names)
                l1 = tf.nn.relu(tf.matmul(tf.concat([lstm_output_reduced, s],1), w1) + b1)

            # second layer
            with tf.variable_scope('l12'):
                w12 = tf.get_variable('w12', [n_l1, n_l1], initializer=w_initializer,
                                         collections=c_names)
                b12 = tf.get_variable('b12', [1, n_l1], initializer=b_initializer, collections=c_names)
                l12 = tf.nn.relu(tf.matmul(l1, w12) + b12)

            # the second layer is different
            if self.dueling:
                # Dueling DQN
                # a single output n_l1 -> 1
                with tf.variable_scope('Value'):
                    w2 = tf.get_variable('w2',[n_l1,1],initializer=w_initializer,collections=c_names)
                    b2 = tf.get_variable('b2',[1,1],initializer=b_initializer,collections=c_names)
                    self.V = tf.matmul(l12,w2) + b2
                # n_l1 -> n_actions
                with tf.variable_scope('Advantage'):
                    w2 = tf.get_variable('w2',[n_l1,self.n_actions],initializer=w_initializer,collections=c_names)
                    b2 = tf.get_variable('b2',[1,self.n_actions],initializer=b_initializer,collections=c_names)
                    self.A = tf.matmul(l12,w2) + b2

                with tf.variable_scope('Q'):
                    out = self.V + (self.A - tf.reduce_mean(self.A,axis=1,keep_dims=True))  # Q = V(s) +A(s,a)

            else:
                with tf.variable_scope('Q'):
                    w2 = tf.get_variable('w2', [n_l1, self.n_actions], initializer=w_initializer, collections=c_names)
                    b2 = tf.get_variable('b2', [1, self.n_actions], initializer=b_initializer, collections=c_names)
                    out = tf.matmul(l1, w2) + b2

            return out

        # input for eval_net
        self.s = tf.placeholder(tf.float32,[None,self.n_features], name = 's')  # state (observation)
        self.lstm_s = tf.placeholder(tf.float32,[None,self.n_lstm_step,self.n_lstm_state], name='lstm1_s')

        self.q_target = tf.placeholder(tf.float32,[None,self.n_actions], name = 'Q_target') # q_target

        # input for target_net
        self.s_ = tf.placeholder(tf.float32, [None, self.n_features], name='s_')
        self.lstm_s_ = tf.placeholder(tf.float32,[None,self.n_lstm_step,self.n_lstm_state], name='lstm1_s_')

        # generate EVAL_NET, update parameters
        with tf.variable_scope('eval_net'):

            # c_names(collections_names), will be used when update target_net
            # tf.random_normal_initializer(mean=0.0, stddev=1.0, seed=None, dtype=tf.float32), return a initializer
            c_names, n_l1, n_lstm, w_initializer, b_initializer =  \
                ['eval_net_params', tf.GraphKeys.GLOBAL_VARIABLES], self.N_L1, self.N_lstm,\
                tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1)  # config of layers

            # input (n_feature) -> l1 (n_l1) -> l2 (n_actions)
            self.q_eval = build_layers(self.s, self.lstm_s, c_names, n_l1, n_lstm, w_initializer, b_initializer)

        # generate TARGET_NET
        with tf.variable_scope('target_net'):
            c_names = ['target_net_params', tf.GraphKeys.GLOBAL_VARIABLES]

            self.q_next = build_layers(self.s_, self.lstm_s_, c_names, n_l1, n_lstm, w_initializer, b_initializer)

        # loss and train
        with tf.variable_scope('loss'):
            self.loss = tf.reduce_mean(tf.squared_difference(self.q_target,self.q_eval))
        with tf.variable_scope('train'):
            self._train_op = tf.train.RMSPropOptimizer(self.lr).minimize(self.loss)

    def store_transition(self, s, lstm_s,  a, r, s_, lstm_s_):
        # RL.store_transition(observation,action,reward,observation_)
        # hasattr(object, name), if object has name attribute
        if not hasattr(self, 'memory_counter'):
            self.memory_counter = 0

        # store np.hstack((s, [a, r], s_, lstm_s, lstm_s_))
        transition = np.hstack((s, [a, r], s_, lstm_s, lstm_s_))  # stack in horizontal direction

        # if memory overflows, replace old memory with new one
        index = self.memory_counter % self.memory_size
        # print(transition)
        self.memory[index, :] = transition
        self.memory_counter += 1

    def update_lstm(self, lstm_s):

        self.lstm_history.append(lstm_s)

    def choose_action(self, observation):
        # the shape of the observation (1, size_of_observation)
        # x1 = np.array([1, 2, 3, 4, 5]), x1_new = x1[np.newaxis, :], now, the shape of x1_new is (1, 5)
        observation = observation[np.newaxis, :]

        if np.random.uniform() < self.epsilon:

            # lstm only contains history, there is no current observation
            lstm_observation = np.array(self.lstm_history)

            actions_value = self.sess.run(self.q_eval,
                                          feed_dict={self.s: observation,
                                                     self.lstm_s: lstm_observation.reshape(1, self.n_lstm_step,
                                                                                           self.n_lstm_state),
                                                     })

            self.store_q_value.append({'observation': observation, 'q_value': actions_value})

            action = np.argmax(actions_value)

        else:
            action = np.random.randint(1, self.n_actions)   
        return action

    def learn(self):

        # check if replace target_net parameters
        if self.learn_step_counter % self.replace_target_iter == 0:
            # run the self.replace_target_op in __int__
            self.sess.run(self.replace_target_op)
            print('Network_parameter_updated\n')

        # randomly pick [batch_size] memory from memory np.hstack((s, [a, r], s_, lstm_s, lstm_s_))
        if self.memory_counter > self.memory_size:
            sample_index = np.random.choice(self.memory_size - self.n_lstm_step, size=self.batch_size)
        else:
            sample_index = np.random.choice(self.memory_counter - self.n_lstm_step, size=self.batch_size)\

        #  transition = np.hstack(s, [a, r], s_, lstm_s, lstm_s_)
        batch_memory = self.memory[sample_index, :self.n_features+1+1+self.n_features]
        lstm_batch_memory = np.zeros([self.batch_size, self.n_lstm_step, self.n_lstm_state * 2])
        for ii in range(len(sample_index)):
            for jj in range(self.n_lstm_step):
                lstm_batch_memory[ii,jj,:] = self.memory[sample_index[ii]+jj,
                                              self.n_features+1+1+self.n_features:]

        # obtain q_next (from target_net) (to q_target) and q_eval (from eval_net)
        # minimize（target_q - q_eval）^2
        # q_target = reward + gamma * q_next
        # in the size of bacth_memory
        # q_next, given the next state from batch, what will be the q_next from q_next
        # q_eval4next, given the next state from batch, what will be the q_eval4next from q_eval
        q_next, q_eval4next = self.sess.run(
            [self.q_next, self.q_eval],  # output
            feed_dict={
                # [s, a, r, s_]
                # input for target_q (last)
                self.s_: batch_memory[:, -self.n_features:], self.lstm_s_: lstm_batch_memory[:,:,self.n_lstm_state:],
                # input for eval_q (last)
                self.s: batch_memory[:, -self.n_features:], self.lstm_s: lstm_batch_memory[:,:,self.n_lstm_state:],
            }
        )
        # q_eval, given the current state from batch, what will be the q_eval from q_eval
        q_eval = self.sess.run(self.q_eval, {self.s: batch_memory[:, :self.n_features],
                                                 self.lstm_s: lstm_batch_memory[:,:,:self.n_lstm_state]})
        q_target = q_eval.copy()
        batch_index = np.arange(self.batch_size, dtype=np.int32)
        eval_act_index = batch_memory[:, self.n_features].astype(int)  # action with a single value (int action)
        reward = batch_memory[:, self.n_features + 1]  # reward with a single value

        # update the q_target at the particular batch at the correponding action
        if self.double_q:
            max_act4next = np.argmax(q_eval4next, axis=1)
            selected_q_next = q_next[batch_index, max_act4next]
        else:
            selected_q_next = np.max(q_next, axis=1)

        q_target[batch_index, eval_act_index] = reward + self.gamma * selected_q_next

        # both self.s and self.q_target belong to eval_q
        # input self.s and self.q_target, output self._train_op, self.loss (to minimize the gap)
        # self.sess.run: given input (feed), output the required element
        _, self.cost = self.sess.run([self._train_op, self.loss],
                                     feed_dict={self.s: batch_memory[:, :self.n_features],
                                                self.lstm_s: lstm_batch_memory[:, :, :self.n_lstm_state],
                                                self.q_target: q_target})

        # gradually increase epsilon
        self.epsilon = self.epsilon + self.epsilon_increment if self.epsilon < self.epsilon_max else self.epsilon_max
        self.learn_step_counter += 1

        return self.cost

    def do_store_reward(self, episode, time, reward):
        while episode >= len(self.reward_store):
            self.reward_store.append(np.zeros([self.n_time]))
        self.reward_store[episode][time] = reward

    def do_store_action(self,episode,time, action):
        while episode >= len(self.action_store):
            self.action_store.append(- np.ones([self.n_time]))
        self.action_store[episode][time] = action

    def do_store_delay(self, episode, time, delay):
        while episode >= len(self.delay_store):
            self.delay_store.append(np.zeros([self.n_time]))
        self.delay_store[episode][time] = delay


    def do_store_energy(self, episode, time, energy, energy2, energy3, energy4):
    
        fog_energy = 0
        for i in range(len(energy3)):
            if energy3[i] != 0:
                fog_energy = energy3[i]


        idle_energy = 0
        for i in range(len(energy4)):
            if energy4[i] != 0:
                idle_energy = energy4[i]

        while episode >= len(self.energy_store):
            self.energy_store.append(np.zeros([self.n_time]))
        self.energy_store[episode][time] = energy + energy2 + fog_energy + idle_energy

    def Initialize(self,sess,iot):
        self.sess = sess
        #self.sess.run(tf.global_variables_initializer())
        self.load_model(iot)


    def load_model(self,iot):
        latest_ckpt = tf.train.latest_checkpoint("./TrainedModel_20UE_2EN_PerformanceMode/800/"+str(iot)+"_X_model")

        print(latest_ckpt, "_____+______________________________________________")
        if latest_ckpt is not None:
            self.saver.restore(self.sess, latest_ckpt)
Config.py
class Config(object):
    
    # System setup
    N_UE             = 20                   # Number of Mobile Devices
    N_EDGE           = 2                    # Number of Edge Servers
    UE_COMP_CAP      = 2.6                  # Mobile Device Computation Capacity
    UE_TRAN_CAP      = 14                   # Mobile Device Transmission Capacity
    EDGE_COMP_CAP    = 42                   # Edge Servers Computation Capacity

    # Energy consumption settings
    UE_ENERGY_STATE  = [0.25, 0.50, 0.75]   # Ultra-power-saving mode, Power-saving mode, Performance mode
    UE_COMP_ENERGY   = 2                    # Computation Power of Mobile Device
    UE_TRAN_ENERGY   = 2.3                  # Transmission Power of Mobile Device
    UE_IDLE_ENERGY   = 0.1                  # Standby power of Mobile Device
    EDGE_COMP_ENERGY = 5                    # Computation Power of Edge Server

    # Task Requrement
    TASK_COMP_DENS   = [0.197, 0.297, 0.397]      # Task Computation Density
    
    #TASK_COMP_DENS   = 0.297


    TASK_MIN_SIZE    = 1
    TASK_MAX_SIZE    = 7
    N_COMPONENT      = 1                    # Number of Task Partitions
    MAX_DELAY        = 10


    # Simulation scenario
    N_EPISODE        = 1000                 # Number of Episodes
    N_TIME_SLOT      = 100                  # Number of Time Slots
    DURATION         = 0.1                  # Time Slot Duration
    TASK_ARRIVE_PROB = 0.3                  # Task Generation Probability
    N_TIME = N_TIME_SLOT + MAX_DELAY


    # Algorithm settings
    LEARNING_RATE    = 0.01
    REWARD_DECAY     = 0.9
    E_GREEDY         = 0.99
    N_NETWORK_UPDATE = 200                  # Networks Parameter Replace
    MEMORY_SIZE      = 500                  # Replay Buffer Memory Size



these are all the codes of this project 